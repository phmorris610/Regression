{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF9kYMSxkrDO4iST8C5fa/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phmorris610/Regression/blob/main/RegressionHousingData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##In this notebook I will do a multiple linear regression using Housing Price Data."
      ],
      "metadata": {
        "id": "R89lmM1xmXJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class regModel:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y"
      ],
      "metadata": {
        "id": "MhTseR4Omf1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First I will define a model with a reModel class"
      ],
      "metadata": {
        "id": "X3Vc2sxomhB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def reg(self):\n",
        "        from sklearn.linear_model import LinearRegression\n",
        "        import matplotlib.pyplot as plt\n",
        "        linreg_model = LinearRegression()\n",
        "        linreg_model.fit(self.X, self.y)\n",
        "        import statsmodels.api as sm\n",
        "        regModel = sm.OLS(self.y, self.X).fit()\n",
        "        y_pred = linreg_model.predict(self.X)\n",
        "        # and a residual plot\n",
        "        plt.title('Residulas')\n",
        "        plt.scatter(regModel.model.exog[:, 1], regModel.resid)\n",
        "        plt.show()\n",
        "        mse_linreg = ((y_pred ** 2).mean())\n",
        "        print(\"MSE = \", mse_linreg)\n",
        "        print(\"rMSE = \", np.sqrt(mse_linreg))\n",
        "        return regModel.summary()  # finally return a Regression Summary\n",
        "\n",
        "    def chiSquareTest(self, k):\n",
        "        from sklearn.feature_selection import SelectKBest\n",
        "        from sklearn.feature_selection import chi2\n",
        "        bestfeatures = SelectKBest(score_func=chi2, k=k)\n",
        "        fit = bestfeatures.fit(self.X, self.y)\n",
        "        dfscores = pd.DataFrame(fit.scores_)\n",
        "        dfcolumns = pd.DataFrame(self.X.columns)\n",
        "        featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n",
        "        featureScores.columns = ['Specs', 'Score']\n",
        "        return featureScores.nlargest(k, 'Score')\n",
        "\n",
        "    def extraTree(self, n):\n",
        "        from sklearn.ensemble import ExtraTreesClassifier\n",
        "        import matplotlib.pyplot as plt\n",
        "        model = ExtraTreesClassifier()\n",
        "        model.fit(self.X, self.y)\n",
        "        feat_importances = pd.Series(model.feature_importances_, index=self.X.columns)\n",
        "        feat_importances.nlargest(n).plot(kind='barh')\n",
        "        plt.show()\n",
        "        feat = []\n",
        "        feat.append(feat_importances.nlargest(n))\n",
        "        return feat\n",
        "\n",
        "    def heatmap(self):\n",
        "        from sklearn.linear_model import LinearRegression\n",
        "        import seaborn as sns\n",
        "        import matplotlib.pyplot as plt\n",
        "        corrmat = data.corr()\n",
        "        top_corr_feat = corrmat.index\n",
        "        plt.figure(figsize=(20, 20))\n",
        "        g = sns.heatmap(data[top_corr_feat].corr(), annot=True, cmap='RdYlGn')\n",
        "        plt.show()\n",
        "\n",
        "    def trainTestSplit(self, t_size, state):\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        X_train, X_val, y_train, y_val = train_test_split(self.X, self.y, train_size=t_size, random_state=state)\n",
        "        return X_train, X_val, y_train, y_val\n",
        "\n",
        "    def normal(self, feature):\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.hist(self.X[feature], bins=20, edgecolor='black')\n",
        "        plt.show()  # TODO: I would like to somehow spit out a display of all the hists at once of all of my features\n",
        "\n",
        "    def tranform(self, selections, trans):\n",
        "        import numpy as np\n",
        "        X_tlog = self.X[selections].applymap(lambda x: np.log(x+1))\n",
        "        y_tlog = np.log(y)\n",
        "        return X_tlog, y_tlog\n",
        "\n",
        "    def describe(self):\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.scatter(self.X, self.y, alpha=0.3)  #TODO: I would like to display all X's vs the y's\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "HwkwEc5KmnYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen above the regModel class has pretty much everything we need to produce a Regression Model, from defining to feature selection to the finer points of splitting and transforming. Note (some functions are still in progress)"
      ],
      "metadata": {
        "id": "pGrmf4Eam3Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('C:/Users/Paul Morris/Desktop/DeepLearn/Housing.csv')\n",
        "train_data = data\n",
        "null_entries = data.isnull().sum()\n",
        "cols_w_nulls = null_entries[null_entries > 0].index\n",
        "X = train_data.drop(columns=cols_w_nulls).copy()\n",
        "y = train_data.pop('SalePrice')\n",
        "for colname in X.select_dtypes('object').columns:\n",
        "    X[colname], _ = X[colname].factorize()"
      ],
      "metadata": {
        "id": "9SyXbVFlnM00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is loading the data, via a csv, deleting the null entries then splitting the data between X (observations, or explanatory variables), and y (the response)"
      ],
      "metadata": {
        "id": "pNWTRFGcnWSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = regModel(X, y)\n",
        "xTree = regModel(X, y)\n",
        "features.chiSquareTest(30)\n",
        "xTree.extraTree(30)"
      ],
      "metadata": {
        "id": "c0zVUiUfnoZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nows it's important to see which variable are pertinent in explaining response (y) wich will be Sale Price. So I'll produce a list of the top 30 explanatory variables using a Chi Squared Test and the Extra Tree Classifier, both produce the same results but the Extra Tree Classifier is a more pleasant display. The vector of the top 30 is below.\n",
        "[SalePrice       0.053516\n",
        "Id              0.035669\n",
        "GrLivArea       0.034066\n",
        "1stFlrSF        0.032866\n",
        "GarageArea      0.032635\n",
        "MoSold          0.032203\n",
        "BsmtUnfSF       0.032188\n",
        "TotalBsmtSF     0.032173\n",
        "LotArea         0.032163\n",
        "YearBuilt       0.031221\n",
        "YrSold          0.030410\n",
        "YearRemodAdd    0.030267\n",
        "BsmtFinSF1      0.029967\n",
        "OpenPorchSF     0.026133\n",
        "TotRmsAbvGrd    0.026111\n",
        "WoodDeckSF      0.025732\n",
        "Neighborhood    0.025510\n",
        "OverallQual     0.023704\n",
        "Exterior2nd     0.022308\n",
        "Exterior1st     0.021462\n",
        "2ndFlrSF        0.019214\n",
        "OverallCond     0.019014\n",
        "HeatingQC       0.018181\n",
        "BedroomAbvGr    0.018124\n",
        "LotConfig       0.018107\n",
        "MSSubClass      0.016914\n",
        "Fireplaces      0.016889\n",
        "LotShape        0.016859\n",
        "BsmtFullBath    0.016400\n",
        "GarageCars      0.014455\n",
        "dtype: float64]"
      ],
      "metadata": {
        "id": "YTXqzbvPoP-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jkZtzBaMpMbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "featured_selections = ['GrLivArea', 'BsmtFinSF1', 'LotArea', 'GarageArea', '1stFlrSF',\n",
        "                       'YearRemodAdd', 'BsmtFinSF1', 'Neighborhood', 'WoodDeckSF',\n",
        "                       'OverallQual', 'Exterior1st']\n",
        "X_linreg = X[featured_selections].copy()"
      ],
      "metadata": {
        "id": "g-kM77QEp1p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obviously sale price is correlated with itself but this gives us a good baseline regarding a top score\n",
        "maybe we want to go with the variables less than 0.2, now append the featured_selections (also don't include Id,\n",
        "or year sold).\n",
        "Feature selection is done lets see the model"
      ],
      "metadata": {
        "id": "cVbAzg0Ep2_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = regModel(X_linreg, y)"
      ],
      "metadata": {
        "id": "zQjgO7Z7qCyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "looks like judging from the p-Values we can get rid of a few, Exterior 2nd, Total rooms above ground, OpenPorch SF,\n",
        "Year Build, Month Sold (should have extracted that earlier how embarrassing)\n",
        "Neighborhood is borderline, interesting\n",
        "After removing the above explanatory variables, the model improved tremendously. Also the Residual plot doesn't show any glaring signs of skewness."
      ],
      "metadata": {
        "id": "cLnOuO7WqM-k"
      }
    }
  ]
}